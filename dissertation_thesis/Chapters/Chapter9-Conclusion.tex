\chapter{Conclusion} % Main chapter title

\label{Chapter9} % For referencing this chapter elsewhere, use \ref{Chapter9}

\lhead{Chapter 9. \emph{Conclusion}}

The objectives of the given document are to extend the earlier \emph{Research Report} that was covering the background and literature review. The background of the project consists of three main axis. First, the importance of \emph{number representation} and the underlying trade-offs it implies both in terms of power consumptions and memory space. Then, the mechanics of \emph{machine learning} when applied to image classification through renowned architectures and datasets to finally highlight the potential mixed-precision vectors. Finally, the importance of the choice of the \emph{hardware architecture} is presented and FPGA are shown to be an interesting in-between for efficiency and flexibility. In the literature review part, \emph{mixed-precision} is presented then applied to neural network with \emph{quantisation} and deployed to FPGAs with \emph{frameworks}. While the literature review had to be reworked to be recentred here, the chosen project remained the same: the development of a benchmark for quantised neural networks to be deployed on reconfigurable architectures. Several frameworks propose this deployment but few are still in active development or open-source. \emph{FINN} and the network trainer \emph{Brevitas} propose a path to design, train and deploy a quantised neural network on an FPGA board.

The development of a benchmark on top of the \emph{Brevitas} / \emph{FINN} workflow helped to assess the impact of mixed-precision on deployed quantised neural networks. The results present an intuitive relation for configurations using 2-bits precision up to 16-bits precision. The difference in end accuracy when comparing the two is manageable and consists of less than one percent. However, a considerable gain is noticeable in throughput and DRAM bandwidth. Now the results highlighted other more unsettling results. First, the representation using 32-bits precisions outperforms several others in terms of throughput and bandwidth and gathers scores similar to 2 or 4 bits representations. Following the same path, the hardware utilisation of the different configurations is extremely similar. Few differences in terms of LUTs, FFs or BRAMs can be noticed.

While these results are counter-intuitive to some extent, they highlight the need to benchmark more the different quantisation frameworks, whether they are for classic applications (Distiller, PyTorch or TensorFlow extensions, etc.) or specifically revolving around FPGA deployment. The results have been transmitted to the Xilinx Research Team working on the \emph{FINN} / \emph{Brevitas} workflow. They want to investigate the different issues of both the performance of the 32-bits configuration and very similar hardware utilisations between the different configurations.

Future works can be conducted following thee axis. First, the extension of the present benchmark to renowned architectures and datasets. This step will require to wait or participate in the development of the framework as several network configurations are not supported for now. Second, the extension of this benchmark to other frameworks that perform similar actions to deploy neural networks on FPGAs or simply compare the training accuracies with more classic implementations. Finally, as \cite{Bacchus2020} performed, assess the importance of the bit-width of weights and activations separately would help determine a sweet spot for those.
