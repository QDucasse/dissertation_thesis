\chapter{Conclusion}

\label{Chapter9} %For referencing this chapter elsewhere, use \ref{Chapter6}

\lhead{Chapter 9. \emph{Conclusion}}

The objectives of this document are to present the many interests behind reduced- and mixed- precision in modern applications. Literature has been put under examination to determine the interests of mixed-precision, the methods that can be used to put it in action and the applications it can have on state-to-the-art projects and research fields. The most important take on mixed-precision is to get rid of superfluous precision, often set by default by the developer early in the development process, to gain performance while losing a negligible amount of precision. Several methods such as iterative refinement or automated tuning can be used to apply mixed-precision to real-world applications in scientific computations.

The literature review highlights several applications but one has been chosen to be further developed. The case of deep learning applied to reconfigurable architectures can highlight the use of mixed-precision along with a novel piece of hardware that fits the application. The implementation of the process will present important parameters contained in the neural network and present the impact of their size and precision on the overall performance and associated precision. FPGAs are a good fit for this application as each of the parameters can be linked to a tailored space and area in the hardware design.

The next step after this research report is the development of the project itself and the redaction of the dissertation presenting the reasoning behind the decisions taken during the development as well as the results obtained and their implications. Results can and should lead to guidelines as well as recommendations for future works and protections put into place.
