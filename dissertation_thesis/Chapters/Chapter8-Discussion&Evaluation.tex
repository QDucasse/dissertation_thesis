\chapter{Discussion \& Evaluation} % Main chapter title

\label{Chapter8} % For referencing this chapter elsewhere, use \ref{Chapter9}

\lhead{Chapter 8. \emph{Discussion \& Evaluation}}

%----------------------------------------------------------------------------------------
%	SECTION 8.1 - Discussion
%----------------------------------------------------------------------------------------

\section{Discussion}

%----------------------------------------------------------------------------------------
%	SECTION 8.2.1 - Network Architectures
%----------------------------------------------------------------------------------------

\subsection{Network Architectures}

The chosen architectures used during the experiments were supposed to be starting from \emph{LeNet-5} and go through well-known architectures such as \emph{AlexNet}, \emph{VGG-16} or \emph{Mobilenet}. However, while these implementations are able to train on a given dataset thanks to the developed benchmark, many of the implementations could not be exported to the intermediate representation. This is due to the fact that the export method and the delegates it uses are still in active development. Only simple layers successions have been implemented. This is why the only network that managed to run on the FPGA board is the \emph{TFC} (three successive fully-connected layers). This layer is known to have correct accuracy on the \emph{MNIST} dataset and uses only simple layers. The \emph{CNV} network corresponds to a simple CNN interconnected with \texttt{HardTanH} activations. This network has been trained as well but could not be deployed. The other networks are variations of the simple \emph{CNV} network and were not trained due to the amount of time and energy needed while obtaining only the first part of the workflow (through Brevitas) in the end. The focus is put on hardware acceleration and this is why this choice was made.

While the training can be completed for any of the mentioned architectures, the scripts to deploy the other architectures will need to be extended. The extension should not be costly and would only be needed in the second part of the workflow: \emph{ONNX} and \emph{FINN}.

%----------------------------------------------------------------------------------------
%	SECTION 8.2.2 - Datasets
%----------------------------------------------------------------------------------------

\subsection{Datasets}

The architectures from earlier were supposed to go along with \emph{MNIST (or similar)} for the simpler networks, \emph{CIFAR-10} for the intermediate ones and either \emph{SVHN} or \emph{GTSRB} for the more complex architectures. The choice of those networks was made over \emph{ImageNet} due to the amount of time and energy training a network on \emph{ImageNet} would require. GTSRB seems like a correct compromise due to the number of classes it holds (over 40) and the emphasis around autonomous driving. In the end, the trainings were made on \emph{FashionMNIST}, a variation the \emph{MNIST} dataset as well as \emph{CIFAR-10} for the simple CNN. The \emph{GTSRB} dataset has been implemented in the benchmark to be used as it is not available by default in \emph{PyTorch} but any of the already provided datasets in \emph{PyTorch} can be easily added to the benchmark.


%----------------------------------------------------------------------------------------
%	SECTION 8.2.3 - Training Process
%----------------------------------------------------------------------------------------

\subsection{Training Process}

% TODO CORRECT REF

The training process was taken from \cite{} This process consists of a 40 epochs training with modifications to the learning rate (division by 10) at epochs 34 and 37. This training process uses the \emph{ADAM} optimiser such as recommended in \cite{}. The loss function by default is \emph{Cross Entropy} but the \emph{Square-Hinged} has been added as an extension to \emph{PyTorch} in order to be able to handle binarised neural networks. Any optimiser or loss function already defined by \emph{PyTorch} can be easily added to the benchmark.

%----------------------------------------------------------------------------------------
%	SECTION 8.2.4 - Deployment Process
%----------------------------------------------------------------------------------------

\subsection{Deployment Process}

The deployment process depends on two main points: \emph{Transformations} used and \emph{Folding} proposed. As presented earlier, the \emph{transformations} are very \guille{network specific} but several common ones can be extracted. Those transformations are still in active development and many of them will be simplified later on. Once the convolutions will be correctly handled, a higher-level API can be used. On the other hand, the \emph{folding} is extremely \guille{network specific} as well and will also depends on the board used. For now, a folding factor can be chosen arbitrarily and specified by hand in the folding transformations. This is supposed to be handled automatically depending on the hardware resources available and provided. This should be available in later versions of FINN. In the presented experiments, the chosen folding factors was 32 and this means the FPGA will have to run 32 passes to process one image. This uses a little over 50\% of the available hardware resources on the FPGA. A divisibility argument has to be kept and using a folding factor of 16 would require over 110\% of the board resources and would therefore not be able to complete the deployment. This parameter can and should be tuned more finely in the future.

%----------------------------------------------------------------------------------------
%	SECTION 8.2.5 - Weight and Activation Bitwidths
%----------------------------------------------------------------------------------------

\subsection{Weight and Activation Bitwidths}

% TODO REF INPUT
The activation bitwidth used were taken specifically to be comparable. This means that the binarised network would require both slightly different forward functions and another loss function. This is why only activations and weights from 2 to 32 bits (2, 4, 8, 16 and 32) were chosen and used. Contrary to \cite{Bacchus2020}, no difference has been made between the activations and weights bitwidths. This is due to the fact that for now, \emph{FINN} cannot handle quantised neural networks with higher bitwidth for activations than weights. Once the feature is made available, the integration to the benchmark is instantaneous as it should already be possible. In addition to the weights and activations bitwidth, an input bitwidth has been provided to be used on the first layer of the network architecture. This is done to prevent harsh quantisation from degrading the end accuracy too much \cite{}.

%----------------------------------------------------------------------------------------
%	SECTION 8.3 - Evaluation
%----------------------------------------------------------------------------------------

\section{Evaluation}

%----------------------------------------------------------------------------------------
%	SUBSECTION 8.3.1 - Comparison to the literature
%----------------------------------------------------------------------------------------

\subsection{Comparison to the literature}

Comparison \emph{TFC} / \emph{Fashion-MNIST} with other results from the literature.

Comparison \emph{CNV} / \emph{CIFAR-10} with other results from the literature.
%----------------------------------------------------------------------------------------
%	SUBSECTION 8.3.2 - Future Works
%----------------------------------------------------------------------------------------

\subsection{Future Works}

The discussion section presented several points that will need to be extended in the future. First of all, using well-known network architectures is extremely important as they have been thoroughly studied and benchmarked. The same goes for the datasets and requires no effort from the \emph{Brevitas} development team as it is already available in PyTorch. While any network can be trained on any dataset for now, the issue comes from the latter parts of the workflow. The export does not cover all the layer configurations for now and other benchmarks will need to be conducted to cover the new architectures and their coupled datasets. \emph{FINN} is not available to handle every architecture for now but several well-known architectures are planned to be supported. For example, \emph{Mobilenet} should run through the whole cycle by the beginning of September this year. Once the end-to-end flow is working, additional benchmarking will be available at a small development cost. As presented in \cite{Bacchus2020}, the difference between the bitwidth of activations and weights will have to be differentiated in order to be evaluated separately and allow to write guidelines.

Overall, the development of the benchmark highlighted a surprising result that will need explanations from the Xilinx development team because it goes against all the objectives and the underlying idea of reduced precision. This result also shows the importance of benchmarks to assess the different ideas one might have when coming into hardware acceleration. \emph{FINN} is, for now, actively developed and open-source, gathering a growing community. However, the same type of benchmarking could be performed on other FPGA frameworks such as the ones presented in \ref{Chapter3}, \cite{Andri2016, Zhao2016, Venieris2017, Ding2019, Jahanshahi2019}. However, those frameworks have their code source closed and would require a lot more time and effort to investigate and benchmark.
